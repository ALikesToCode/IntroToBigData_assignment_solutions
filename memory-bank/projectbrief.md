# Project Brief: Introduction to Big Data - Assignment Solutions

## Project Overview
This repository contains solutions for weekly assignments in an Introduction to Big Data course. Each week focuses on different big data technologies, frameworks, and implementation patterns.

## Core Requirements
- Weekly assignments building knowledge incrementally
- Emphasis on practical implementation using industry-standard tools
- Both local development and cloud deployment capabilities
- Comprehensive documentation and testing for each solution

## Weekly Structure
- **Week 1**: Basic big data processing and file operations
- **Week 2**: [To be documented]
- **Week 3**: Apache Spark for user click data analysis, local and cloud deployment
- **Week 4**: PySpark SCD Type II implementation on Dataproc cluster
- Additional weeks as assigned

## Deliverables Per Week
- Primary implementation files
- Test scripts for local validation
- Documentation (README.md)
- Requirements/dependencies
- Sample data files
- Cloud deployment scripts (when applicable)

## Technology Stack
- Python 3.7+
- Apache Spark / PySpark
- Google Cloud Platform (Dataproc, GCS)
- Jupyter Notebooks for analysis
- Standard data science libraries (pandas, numpy, matplotlib, seaborn)

## Success Criteria
- Working code that executes successfully
- Both local and cloud deployment capabilities
- Clear documentation with setup instructions
- Proper error handling and edge case management
- Organized code structure following best practices 