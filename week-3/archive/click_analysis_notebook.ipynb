{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# User Click Data Analysis with Apache Spark\n",
        "## Interactive Cloud-Ready Analysis Notebook\n",
        "\n",
        "This notebook provides an interactive environment for analyzing user click data using Apache Spark on Google Cloud Dataproc. It demonstrates distributed computing principles and provides visualization capabilities.\n",
        "\n",
        "**Dataset**: User click events with timestamp and user ID  \n",
        "**Platform**: Google Cloud Dataproc with Jupyter integration  \n",
        "**Objective**: Analyze click patterns across 6-hour time intervals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session with optimized configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"InteractiveClickAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.ui.port\", \"4041\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbose output\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(f\"âœ“ Spark session initialized\")\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Load and Process Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define processing functions\n",
        "def get_time_interval(time_str):\n",
        "    \"\"\"Categorize time into 6-hour intervals.\"\"\"\n",
        "    try:\n",
        "        hour = int(time_str.split(':')[0])\n",
        "        if 0 <= hour < 6:\n",
        "            return \"0-6\"\n",
        "        elif 6 <= hour < 12:\n",
        "            return \"6-12\"\n",
        "        elif 12 <= hour < 18:\n",
        "            return \"12-18\"\n",
        "        elif 18 <= hour < 24:\n",
        "            return \"18-24\"\n",
        "        else:\n",
        "            return \"invalid\"\n",
        "    except:\n",
        "        return \"invalid\"\n",
        "\n",
        "def parse_line(line):\n",
        "    \"\"\"Parse a data line into components.\"\"\"\n",
        "    try:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) == 3:\n",
        "            date_part, time_part, user_id = parts\n",
        "            interval = get_time_interval(time_part)\n",
        "            if interval != \"invalid\":\n",
        "                return (date_part, time_part, user_id, interval)\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# Load and process data\n",
        "data_file = \"data.txt\"\n",
        "raw_rdd = spark.sparkContext.textFile(data_file)\n",
        "\n",
        "print(f\"âœ“ Data loaded from {data_file}\")\n",
        "print(f\"Total lines: {raw_rdd.count()}\")\n",
        "\n",
        "# Process data using RDD transformations\n",
        "parsed_rdd = raw_rdd.map(parse_line).filter(lambda x: x is not None)\n",
        "interval_rdd = parsed_rdd.map(lambda x: (x[3], 1))\n",
        "click_counts = interval_rdd.reduceByKey(lambda a, b: a + b).collect()\n",
        "\n",
        "results_dict = dict(click_counts)\n",
        "print(f\"âœ“ Processed {parsed_rdd.count()} valid records\")\n",
        "\n",
        "# Display results\n",
        "print(\"\\nClick counts by time interval:\")\n",
        "for interval in [\"0-6\", \"6-12\", \"12-18\", \"18-24\"]:\n",
        "    count = results_dict.get(interval, 0)\n",
        "    print(f\"{interval}: {count} clicks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Data Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare data for plotting\n",
        "intervals = [\"0-6\", \"6-12\", \"12-18\", \"18-24\"]\n",
        "counts = [results_dict.get(interval, 0) for interval in intervals]\n",
        "total_clicks = sum(counts)\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "fig.suptitle('User Click Data Analysis Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Bar chart\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "bars = ax1.bar(intervals, counts, color=colors)\n",
        "ax1.set_title('Click Count by Time Interval')\n",
        "ax1.set_xlabel('Time Interval')\n",
        "ax1.set_ylabel('Number of Clicks')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(counts):\n",
        "    ax1.text(i, v + 0.1, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "wedges, texts, autotexts = ax2.pie(counts, labels=intervals, autopct='%1.1f%%', \n",
        "                                   colors=colors, startangle=90)\n",
        "ax2.set_title('Click Distribution by Time Interval')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"ðŸ“Š ANALYSIS SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total Click Events: {total_clicks}\")\n",
        "peak_interval = intervals[counts.index(max(counts))]\n",
        "peak_count = max(counts)\n",
        "peak_percentage = (peak_count / total_clicks) * 100\n",
        "\n",
        "print(f\"Peak Activity Period: {peak_interval} hours\")\n",
        "print(f\"Peak Clicks: {peak_count} ({peak_percentage:.1f}% of total)\")\n",
        "print(\"\\nActivity Distribution:\")\n",
        "for interval, count in zip(intervals, counts):\n",
        "    percentage = (count / total_clicks) * 100\n",
        "    print(f\"  {interval}: {count} clicks ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. DataFrame Analysis with SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert RDD to DataFrame for SQL analysis\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"date\", StringType(), True),\n",
        "    StructField(\"time\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"time_interval\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(parsed_rdd, schema)\n",
        "df.createOrReplaceTempView(\"click_data\")\n",
        "\n",
        "print(\"âœ“ DataFrame created and registered as 'click_data'\")\n",
        "print(f\"DataFrame schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nDataFrame preview:\")\n",
        "df.show(10)\n",
        "\n",
        "# Perform SQL analysis\n",
        "sql_results = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        time_interval,\n",
        "        COUNT(*) as click_count,\n",
        "        COUNT(DISTINCT user_id) as unique_users,\n",
        "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as percentage\n",
        "    FROM click_data \n",
        "    GROUP BY time_interval \n",
        "    ORDER BY time_interval\n",
        "\"\"\")\n",
        "\n",
        "print(\"SQL Analysis Results:\")\n",
        "sql_results.show()\n",
        "\n",
        "# Advanced insights with SQL\n",
        "peak_analysis = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        time_interval,\n",
        "        COUNT(*) as clicks,\n",
        "        COUNT(DISTINCT user_id) as users,\n",
        "        ROUND(COUNT(*) * 1.0 / COUNT(DISTINCT user_id), 2) as clicks_per_user\n",
        "    FROM click_data \n",
        "    GROUP BY time_interval \n",
        "    ORDER BY clicks DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nDetailed Analysis (sorted by activity):\")\n",
        "peak_analysis.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Business Insights & Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive business insights\n",
        "print(\"ðŸŽ¯ BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Key metrics\n",
        "total_events = df.count()\n",
        "unique_users = df.select(\"user_id\").distinct().count()\n",
        "avg_clicks_per_user = total_events / unique_users\n",
        "\n",
        "print(f\"ðŸ“ˆ Key Metrics:\")\n",
        "print(f\"   â€¢ Total Click Events: {total_events}\")\n",
        "print(f\"   â€¢ Unique Users: {unique_users}\")\n",
        "print(f\"   â€¢ Average Clicks per User: {avg_clicks_per_user:.2f}\")\n",
        "print()\n",
        "\n",
        "# Peak activity insights\n",
        "peak_data = peak_analysis.collect()\n",
        "peak_hour = peak_data[0]['time_interval']\n",
        "peak_clicks = peak_data[0]['clicks']\n",
        "\n",
        "print(f\"ðŸ† Peak Activity Analysis:\")\n",
        "print(f\"   â€¢ Peak Period: {peak_hour} hours\")\n",
        "print(f\"   â€¢ Peak Clicks: {peak_clicks} events\")\n",
        "print(f\"   â€¢ Peak Represents: {(peak_clicks/total_events)*100:.1f}% of total activity\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸ’¡ Strategic Recommendations:\")\n",
        "print(f\"   1. Schedule critical campaigns during {peak_hour} hours\")\n",
        "print(f\"   2. Optimize server resources for {peak_hour} peak demand\")\n",
        "print(f\"   3. Plan maintenance during low-activity periods (0-6 hours)\")\n",
        "print(f\"   4. Consider user engagement strategies for off-peak hours\")\n",
        "print()\n",
        "\n",
        "print(\"ðŸ”§ Technical Insights:\")\n",
        "print(f\"   â€¢ Data processing success rate: {(total_events/30)*100:.1f}%\")\n",
        "print(f\"   â€¢ Distributed processing across {spark.sparkContext.defaultParallelism} partitions\")\n",
        "print(f\"   â€¢ Real-time monitoring available at: {spark.sparkContext.uiWebUrl}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Cleanup & Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Stop Spark session when analysis is complete\n",
        "# Uncomment the following line to stop the session\n",
        "# spark.stop()\n",
        "# print(\"âœ“ Spark session stopped\")\n",
        "\n",
        "print(\"ðŸŽ‰ Interactive Analysis Complete!\")\n",
        "print(\"\\nðŸ“ What you've accomplished:\")\n",
        "print(\"  âœ“ Loaded and processed click data with Apache Spark\")\n",
        "print(\"  âœ“ Performed distributed computing analysis\")\n",
        "print(\"  âœ“ Created visualizations of user behavior patterns\")\n",
        "print(\"  âœ“ Generated SQL-based insights\")\n",
        "print(\"  âœ“ Developed actionable business recommendations\")\n",
        "\n",
        "print(\"\\nðŸš€ Next Steps:\")\n",
        "print(\"  â€¢ Experiment with the DataFrames (df, sql_results, peak_analysis)\")\n",
        "print(\"  â€¢ Modify the SQL queries for custom analysis\")\n",
        "print(\"  â€¢ Scale to larger datasets using cloud deployment\")\n",
        "print(\"  â€¢ Integrate with real-time streaming data\")\n",
        "print(f\"  â€¢ Monitor performance via Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "\n",
        "print(\"\\nðŸ’¾ Available Objects:\")\n",
        "print(\"  â€¢ spark: SparkSession for creating new DataFrames\")\n",
        "print(\"  â€¢ df: Main click data DataFrame\")\n",
        "print(\"  â€¢ sql_results: Aggregated statistics\")\n",
        "print(\"  â€¢ peak_analysis: Detailed activity analysis\")\n",
        "print(\"  â€¢ results_dict: RDD analysis results\")\n",
        "\n",
        "print(\"\\nðŸ”— For cloud deployment, consider:\")\n",
        "print(\"  â€¢ Google Cloud Dataproc\")\n",
        "print(\"  â€¢ AWS EMR\")\n",
        "print(\"  â€¢ Azure HDInsight\")\n",
        "print(\"  â€¢ Databricks\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
