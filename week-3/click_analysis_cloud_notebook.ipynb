{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🚀 Click Data Analysis with Apache Spark on Google Cloud\n",
        "\n",
        "This notebook demonstrates distributed data processing using Apache Spark on Google Cloud Dataproc.\n",
        "\n",
        "## 📊 Week-3 Assignment: Big Data Processing Evolution\n",
        "- **Week 1**: Sequential processing with Python + GCS API\n",
        "- **Week 2**: Event-driven processing with Cloud Functions\n",
        "- **Week 3**: Distributed processing with Apache Spark ⭐\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall numpy pandas scipy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, desc, when, split, hour\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Configure display\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"📚 Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CloudClickAnalysis-Jupyter\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"✅ Spark version: {spark.version}\")\n",
        "print(f\"🌐 Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(f\"📊 Spark Context: {spark.sparkContext}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure paths (update bucket name as needed)\n",
        "BUCKET_NAME = \"spark-click-analysis-20250629-231200-unique\"  # Replace with your bucket\n",
        "input_path = f\"gs://{BUCKET_NAME}/input/data.txt\"\n",
        "output_path = f\"gs://{BUCKET_NAME}/output/\"\n",
        "\n",
        "print(f\"📂 Input: {input_path}\")\n",
        "print(f\"💾 Output: {output_path}\")\n",
        "\n",
        "# Load and parse the click data\n",
        "print(\"📊 Loading click data from GCS...\")\n",
        "\n",
        "# Read as text and parse the format: \"10-Jan 11:10 1001\"\n",
        "df_raw = spark.read.text(input_path)\n",
        "\n",
        "df_parsed = df_raw.select(\n",
        "    split(col(\"value\"), \" \").alias(\"parts\")\n",
        ").select(\n",
        "    col(\"parts\")[0].alias(\"date_part\"),\n",
        "    col(\"parts\")[1].alias(\"time_part\"), \n",
        "    col(\"parts\")[2].alias(\"user_id\")\n",
        ").filter(\n",
        "    col(\"date_part\").isNotNull() & \n",
        "    col(\"time_part\").isNotNull() & \n",
        "    col(\"user_id\").isNotNull()\n",
        ")\n",
        "\n",
        "# Extract hour from time\n",
        "df_clicks = df_parsed.withColumn(\n",
        "    \"hour\",\n",
        "    split(col(\"time_part\"), \":\")[0].cast(\"int\")\n",
        ")\n",
        "\n",
        "total_records = df_raw.count()\n",
        "valid_records = df_clicks.count()\n",
        "\n",
        "print(f\"📈 Total records: {total_records}\")\n",
        "print(f\"✅ Valid records: {valid_records}\")\n",
        "print(f\"📊 Success rate: {(valid_records/total_records)*100:.1f}%\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"📋 Sample Data:\")\n",
        "df_clicks.select(\"date_part\", \"time_part\", \"user_id\", \"hour\").show(10, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-based analysis (6-hour intervals)\n",
        "df_with_intervals = df_clicks.withColumn(\n",
        "    \"time_interval\",\n",
        "    when((col(\"hour\") >= 0) & (col(\"hour\") < 6), \"00-06 Night\")\n",
        "    .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"06-12 Morning\")\n",
        "    .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"12-18 Afternoon\")\n",
        "    .when((col(\"hour\") >= 18) & (col(\"hour\") < 24), \"18-24 Evening\")\n",
        "    .otherwise(\"Unknown\")\n",
        ")\n",
        "\n",
        "# Analyze time intervals\n",
        "interval_analysis = df_with_intervals.groupBy(\"time_interval\") \\\n",
        "    .agg(count(\"*\").alias(\"click_count\")) \\\n",
        "    .orderBy(desc(\"click_count\"))\n",
        "\n",
        "print(\"📊 Time Interval Analysis:\")\n",
        "interval_analysis.show()\n",
        "\n",
        "# User activity analysis\n",
        "user_analysis = df_clicks.groupBy(\"user_id\") \\\n",
        "    .agg(count(\"*\").alias(\"click_count\")) \\\n",
        "    .orderBy(desc(\"click_count\"))\n",
        "\n",
        "print(\"👥 Top User Activity:\")\n",
        "user_analysis.show(10)\n",
        "\n",
        "# Generate summary\n",
        "interval_results = interval_analysis.collect()\n",
        "total_clicks = sum(row.click_count for row in interval_results)\n",
        "peak_interval = interval_results[0]\n",
        "\n",
        "print(f\"\\n🎯 ANALYSIS SUMMARY:\")\n",
        "print(f\"📊 Total clicks: {total_clicks}\")\n",
        "print(f\"👥 Unique users: {user_analysis.count()}\")\n",
        "print(f\"🏆 Peak activity: {peak_interval['time_interval']} ({peak_interval['click_count']} clicks)\")\n",
        "print(f\"📈 Peak percentage: {(peak_interval['click_count']/total_clicks)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n✅ Interactive analysis ready! Modify and run cells to explore the data further.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
